{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize,sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Evolving technology continuously reshapes our world, pushing boundaries and unlocking new possibilities. From the rapid advancements in artificial intelligence and machine learning that augment our capabilities, to the transformative potential of quantum computing and biotechnology, innovation propels us forward. Each breakthrough in connectivity, like 5 G networks, fuels global integration and communication. Smart devices and the Internet of Things (IoT) intertwine with everyday life, enhancing efficiency and convenience. As technology evolves, it not only accelerates progress but also challenges us to navigate its ethical and societal implications, shaping the future we collectively forge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Evolving technology continuously reshapes our world, pushing boundaries and unlocking new possibilities. From the rapid advancements in artificial intelligence and machine learning that augment our capabilities, to the transformative potential of quantum computing and biotechnology, innovation propels us forward. Each breakthrough in connectivity, like 5 G networks, fuels global integration and communication. Smart devices and the Internet of Things (IoT) intertwine with everyday life, enhancing efficiency and convenience. As technology evolves, it not only accelerates progress but also challenges us to navigate its ethical and societal implications, shaping the future we collectively forge.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evolving technology continuously reshapes our world, pushing boundaries and unlocking new possibilities. from the rapid advancements in artificial intelligence and machine learning that augment our capabilities, to the transformative potential of quantum computing and biotechnology, innovation propels us forward. each breakthrough in connectivity, like 5 g networks, fuels global integration and communication. smart devices and the internet of things (iot) intertwine with everyday life, enhancing efficiency and convenience. as technology evolves, it not only accelerates progress but also challenges us to navigate its ethical and societal implications, shaping the future we collectively forge.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Evolving technology continuously reshapes our world, pushing boundaries and unlocking new possibilities.',\n",
       " 'From the rapid advancements in artificial intelligence and machine learning that augment our capabilities, to the transformative potential of quantum computing and biotechnology, innovation propels us forward.',\n",
       " 'Each breakthrough in connectivity, like 5 G networks, fuels global integration and communication.',\n",
       " 'Smart devices and the Internet of Things (IoT) intertwine with everyday life, enhancing efficiency and convenience.',\n",
       " 'As technology evolves, it not only accelerates progress but also challenges us to navigate its ethical and societal implications, shaping the future we collectively forge.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize=nltk.sent_tokenize(text)\n",
    "sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Evolving',\n",
       " 'technology',\n",
       " 'continuously',\n",
       " 'reshapes',\n",
       " 'our',\n",
       " 'world',\n",
       " ',',\n",
       " 'pushing',\n",
       " 'boundaries',\n",
       " 'and',\n",
       " 'unlocking',\n",
       " 'new',\n",
       " 'possibilities',\n",
       " '.',\n",
       " 'From',\n",
       " 'the',\n",
       " 'rapid',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'that',\n",
       " 'augment',\n",
       " 'our',\n",
       " 'capabilities',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'transformative',\n",
       " 'potential',\n",
       " 'of',\n",
       " 'quantum',\n",
       " 'computing',\n",
       " 'and',\n",
       " 'biotechnology',\n",
       " ',',\n",
       " 'innovation',\n",
       " 'propels',\n",
       " 'us',\n",
       " 'forward',\n",
       " '.',\n",
       " 'Each',\n",
       " 'breakthrough',\n",
       " 'in',\n",
       " 'connectivity',\n",
       " ',',\n",
       " 'like',\n",
       " '5',\n",
       " 'G',\n",
       " 'networks',\n",
       " ',',\n",
       " 'fuels',\n",
       " 'global',\n",
       " 'integration',\n",
       " 'and',\n",
       " 'communication',\n",
       " '.',\n",
       " 'Smart',\n",
       " 'devices',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Internet',\n",
       " 'of',\n",
       " 'Things',\n",
       " '(',\n",
       " 'IoT',\n",
       " ')',\n",
       " 'intertwine',\n",
       " 'with',\n",
       " 'everyday',\n",
       " 'life',\n",
       " ',',\n",
       " 'enhancing',\n",
       " 'efficiency',\n",
       " 'and',\n",
       " 'convenience',\n",
       " '.',\n",
       " 'As',\n",
       " 'technology',\n",
       " 'evolves',\n",
       " ',',\n",
       " 'it',\n",
       " 'not',\n",
       " 'only',\n",
       " 'accelerates',\n",
       " 'progress',\n",
       " 'but',\n",
       " 'also',\n",
       " 'challenges',\n",
       " 'us',\n",
       " 'to',\n",
       " 'navigate',\n",
       " 'its',\n",
       " 'ethical',\n",
       " 'and',\n",
       " 'societal',\n",
       " 'implications',\n",
       " ',',\n",
       " 'shaping',\n",
       " 'the',\n",
       " 'future',\n",
       " 'we',\n",
       " 'collectively',\n",
       " 'forge',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize=nltk.word_tokenize(text)\n",
    "word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer,PorterStemmer\n",
    "ss=SnowballStemmer('english')\n",
    "po=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemss=[]\n",
    "for i in word_tokenize:\n",
    "    stemss.append(ss.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['evolv',\n",
       " 'technolog',\n",
       " 'continu',\n",
       " 'reshap',\n",
       " 'our',\n",
       " 'world',\n",
       " ',',\n",
       " 'push',\n",
       " 'boundari',\n",
       " 'and',\n",
       " 'unlock',\n",
       " 'new',\n",
       " 'possibl',\n",
       " '.',\n",
       " 'from',\n",
       " 'the',\n",
       " 'rapid',\n",
       " 'advanc',\n",
       " 'in',\n",
       " 'artifici',\n",
       " 'intellig',\n",
       " 'and',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'that',\n",
       " 'augment',\n",
       " 'our',\n",
       " 'capabl',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'transform',\n",
       " 'potenti',\n",
       " 'of',\n",
       " 'quantum',\n",
       " 'comput',\n",
       " 'and',\n",
       " 'biotechnolog',\n",
       " ',',\n",
       " 'innov',\n",
       " 'propel',\n",
       " 'us',\n",
       " 'forward',\n",
       " '.',\n",
       " 'each',\n",
       " 'breakthrough',\n",
       " 'in',\n",
       " 'connect',\n",
       " ',',\n",
       " 'like',\n",
       " '5',\n",
       " 'g',\n",
       " 'network',\n",
       " ',',\n",
       " 'fuel',\n",
       " 'global',\n",
       " 'integr',\n",
       " 'and',\n",
       " 'communic',\n",
       " '.',\n",
       " 'smart',\n",
       " 'devic',\n",
       " 'and',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'of',\n",
       " 'thing',\n",
       " '(',\n",
       " 'iot',\n",
       " ')',\n",
       " 'intertwin',\n",
       " 'with',\n",
       " 'everyday',\n",
       " 'life',\n",
       " ',',\n",
       " 'enhanc',\n",
       " 'effici',\n",
       " 'and',\n",
       " 'conveni',\n",
       " '.',\n",
       " 'as',\n",
       " 'technolog',\n",
       " 'evolv',\n",
       " ',',\n",
       " 'it',\n",
       " 'not',\n",
       " 'onli',\n",
       " 'acceler',\n",
       " 'progress',\n",
       " 'but',\n",
       " 'also',\n",
       " 'challeng',\n",
       " 'us',\n",
       " 'to',\n",
       " 'navig',\n",
       " 'it',\n",
       " 'ethic',\n",
       " 'and',\n",
       " 'societ',\n",
       " 'implic',\n",
       " ',',\n",
       " 'shape',\n",
       " 'the',\n",
       " 'futur',\n",
       " 'we',\n",
       " 'collect',\n",
       " 'forg',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stempo=[]\n",
    "for i in word_tokenize:\n",
    "    stempo.append(po.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['evolv',\n",
       " 'technolog',\n",
       " 'continu',\n",
       " 'reshap',\n",
       " 'our',\n",
       " 'world',\n",
       " ',',\n",
       " 'push',\n",
       " 'boundari',\n",
       " 'and',\n",
       " 'unlock',\n",
       " 'new',\n",
       " 'possibl',\n",
       " '.',\n",
       " 'from',\n",
       " 'the',\n",
       " 'rapid',\n",
       " 'advanc',\n",
       " 'in',\n",
       " 'artifici',\n",
       " 'intellig',\n",
       " 'and',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'that',\n",
       " 'augment',\n",
       " 'our',\n",
       " 'capabl',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'transform',\n",
       " 'potenti',\n",
       " 'of',\n",
       " 'quantum',\n",
       " 'comput',\n",
       " 'and',\n",
       " 'biotechnolog',\n",
       " ',',\n",
       " 'innov',\n",
       " 'propel',\n",
       " 'us',\n",
       " 'forward',\n",
       " '.',\n",
       " 'each',\n",
       " 'breakthrough',\n",
       " 'in',\n",
       " 'connect',\n",
       " ',',\n",
       " 'like',\n",
       " '5',\n",
       " 'g',\n",
       " 'network',\n",
       " ',',\n",
       " 'fuel',\n",
       " 'global',\n",
       " 'integr',\n",
       " 'and',\n",
       " 'commun',\n",
       " '.',\n",
       " 'smart',\n",
       " 'devic',\n",
       " 'and',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'of',\n",
       " 'thing',\n",
       " '(',\n",
       " 'iot',\n",
       " ')',\n",
       " 'intertwin',\n",
       " 'with',\n",
       " 'everyday',\n",
       " 'life',\n",
       " ',',\n",
       " 'enhanc',\n",
       " 'effici',\n",
       " 'and',\n",
       " 'conveni',\n",
       " '.',\n",
       " 'as',\n",
       " 'technolog',\n",
       " 'evolv',\n",
       " ',',\n",
       " 'it',\n",
       " 'not',\n",
       " 'onli',\n",
       " 'acceler',\n",
       " 'progress',\n",
       " 'but',\n",
       " 'also',\n",
       " 'challeng',\n",
       " 'us',\n",
       " 'to',\n",
       " 'navig',\n",
       " 'it',\n",
       " 'ethic',\n",
       " 'and',\n",
       " 'societ',\n",
       " 'implic',\n",
       " ',',\n",
       " 'shape',\n",
       " 'the',\n",
       " 'futur',\n",
       " 'we',\n",
       " 'collect',\n",
       " 'forg',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['evolv', 'technolog', 'continu', 'reshap', 'our', 'world', ',', 'push', 'boundari', 'and', 'unlock', 'new', 'possibl', '.', 'from', 'the', 'rapid', 'advanc', 'in', 'artifici', 'intellig', 'and', 'machin', 'learn', 'that', 'augment', 'our', 'capabl', ',', 'to', 'the', 'transform', 'potenti', 'of', 'quantum', 'comput', 'and', 'biotechnolog', ',', 'innov', 'propel', 'u', 'forward', '.', 'each', 'breakthrough', 'in', 'connect', ',', 'like', '5', 'g', 'network', ',', 'fuel', 'global', 'integr', 'and', 'communic', '.', 'smart', 'devic', 'and', 'the', 'internet', 'of', 'thing', '(', 'iot', ')', 'intertwin', 'with', 'everyday', 'life', ',', 'enhanc', 'effici', 'and', 'conveni', '.', 'a', 'technolog', 'evolv', ',', 'it', 'not', 'onli', 'acceler', 'progress', 'but', 'also', 'challeng', 'u', 'to', 'navig', 'it', 'ethic', 'and', 'societ', 'implic', ',', 'shape', 'the', 'futur', 'we', 'collect', 'forg', '.']\n"
     ]
    }
   ],
   "source": [
    "lem =[]\n",
    "for i in stemss:\n",
    "    lemetized_word = lemmatizer.lemmatize(i)\n",
    "    lem.append(lemetized_word)\n",
    "print(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['evolv',\n",
       " 'technolog',\n",
       " 'continu',\n",
       " 'reshap',\n",
       " 'our',\n",
       " 'world',\n",
       " 'push',\n",
       " 'boundari',\n",
       " 'and',\n",
       " 'unlock',\n",
       " 'new',\n",
       " 'possibl',\n",
       " 'from',\n",
       " 'the',\n",
       " 'rapid',\n",
       " 'advanc',\n",
       " 'in',\n",
       " 'artifici',\n",
       " 'intellig',\n",
       " 'and',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'that',\n",
       " 'augment',\n",
       " 'our',\n",
       " 'capabl',\n",
       " 'to',\n",
       " 'the',\n",
       " 'transform',\n",
       " 'potenti',\n",
       " 'of',\n",
       " 'quantum',\n",
       " 'comput',\n",
       " 'and',\n",
       " 'biotechnolog',\n",
       " 'innov',\n",
       " 'propel',\n",
       " 'u',\n",
       " 'forward',\n",
       " 'each',\n",
       " 'breakthrough',\n",
       " 'in',\n",
       " 'connect',\n",
       " 'like',\n",
       " 'g',\n",
       " 'network',\n",
       " 'fuel',\n",
       " 'global',\n",
       " 'integr',\n",
       " 'and',\n",
       " 'communic',\n",
       " 'smart',\n",
       " 'devic',\n",
       " 'and',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'of',\n",
       " 'thing',\n",
       " 'iot',\n",
       " 'intertwin',\n",
       " 'with',\n",
       " 'everyday',\n",
       " 'life',\n",
       " 'enhanc',\n",
       " 'effici',\n",
       " 'and',\n",
       " 'conveni',\n",
       " 'a',\n",
       " 'technolog',\n",
       " 'evolv',\n",
       " 'it',\n",
       " 'not',\n",
       " 'onli',\n",
       " 'acceler',\n",
       " 'progress',\n",
       " 'but',\n",
       " 'also',\n",
       " 'challeng',\n",
       " 'u',\n",
       " 'to',\n",
       " 'navig',\n",
       " 'it',\n",
       " 'ethic',\n",
       " 'and',\n",
       " 'societ',\n",
       " 'implic',\n",
       " 'shape',\n",
       " 'the',\n",
       " 'futur',\n",
       " 'we',\n",
       " 'collect',\n",
       " 'forg']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_without_punctuation=[]\n",
    "for i in lem:\n",
    "    if i.isalpha():\n",
    "        word_without_punctuation.append(i)\n",
    "        \n",
    "\n",
    "word_without_punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part of speech postagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts of Speech:  [('evolv', 'JJ'), ('technolog', 'NN'), ('continu', 'NN'), ('reshap', 'VB'), ('our', 'PRP$'), ('world', 'NN'), (',', ','), ('push', 'NN'), ('boundari', 'NN'), ('and', 'CC'), ('unlock', 'JJ'), ('new', 'JJ'), ('possibl', 'NN'), ('.', '.'), ('from', 'IN'), ('the', 'DT'), ('rapid', 'JJ'), ('advanc', 'NN'), ('in', 'IN'), ('artifici', 'JJ'), ('intellig', 'NN'), ('and', 'CC'), ('machin', 'NN'), ('learn', 'VBP'), ('that', 'IN'), ('augment', 'JJ'), ('our', 'PRP$'), ('capabl', 'NN'), (',', ','), ('to', 'TO'), ('the', 'DT'), ('transform', 'NN'), ('potenti', 'NN'), ('of', 'IN'), ('quantum', 'NN'), ('comput', 'NN'), ('and', 'CC'), ('biotechnolog', 'NN'), (',', ','), ('innov', 'JJ'), ('propel', 'NN'), ('u', 'JJ'), ('forward', 'RB'), ('.', '.'), ('each', 'DT'), ('breakthrough', 'NN'), ('in', 'IN'), ('connect', 'NN'), (',', ','), ('like', 'IN'), ('5', 'CD'), ('g', 'NN'), ('network', 'NN'), (',', ','), ('fuel', 'NN'), ('global', 'JJ'), ('integr', 'NN'), ('and', 'CC'), ('communic', 'NN'), ('.', '.'), ('smart', 'JJ'), ('devic', 'NN'), ('and', 'CC'), ('the', 'DT'), ('internet', 'NN'), ('of', 'IN'), ('thing', 'NN'), ('(', '('), ('iot', 'NN'), (')', ')'), ('intertwin', 'NN'), ('with', 'IN'), ('everyday', 'JJ'), ('life', 'NN'), (',', ','), ('enhanc', 'NN'), ('effici', 'NN'), ('and', 'CC'), ('conveni', 'NN'), ('.', '.'), ('a', 'DT'), ('technolog', 'NN'), ('evolv', 'NN'), (',', ','), ('it', 'PRP'), ('not', 'RB'), ('onli', 'VBZ'), ('acceler', 'JJ'), ('progress', 'NN'), ('but', 'CC'), ('also', 'RB'), ('challeng', 'VBP'), ('u', 'JJ'), ('to', 'TO'), ('navig', 'VB'), ('it', 'PRP'), ('ethic', 'JJ'), ('and', 'CC'), ('societ', 'JJ'), ('implic', 'NN'), (',', ','), ('shape', 'VBP'), ('the', 'DT'), ('futur', 'NN'), ('we', 'PRP'), ('collect', 'VBP'), ('forg', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Parts of Speech: \",nltk.pos_tag(lem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "sw_nltk = stopwords.words('english')\n",
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test with numbers like one hundred and twenty-three and four hundred and fifty-six.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from num2words import num2words\n",
    "\n",
    "def convert_numbers_to_words(text):\n",
    "    pattern = r'\\d+'  \n",
    "    \n",
    "    def replace(match):\n",
    "        number = int(match.group()) \n",
    "        return num2words(number)  \n",
    "    \n",
    "\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)  \n",
    "\n",
    "    result = re.sub(pattern, replace, text)\n",
    "    \n",
    "    return result\n",
    "\n",
    "word_without_punctuation = ['This', 'is', 'a', 'test', 'with', 'numbers', 'like', '123', 'and', '456.']\n",
    "\n",
    "text_converted = convert_numbers_to_words(word_without_punctuation)\n",
    "\n",
    "print(text_converted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
